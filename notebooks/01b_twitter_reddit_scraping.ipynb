{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e2fb9b-ee74-4930-b2ba-6c0f0d762fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObjective:\\nCollect and store financial posts and comments related to specific stocks from:\\n1. Twitter (via X API or third-party dataset)\\n2. Reddit (via Pushshift API or PRAW)\\n\\nOutcome:\\n- CSV files with text, author, date, source, and possibly metadata like upvotes or likes.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VesterAI - Notebook 01b: Twitter and Reddit Scraping\n",
    "\n",
    "\"\"\"\n",
    "Objective:\n",
    "Collect and store financial posts and comments related to specific stocks from:\n",
    "1. Twitter (via X API or third-party dataset)\n",
    "2. Reddit (via Pushshift API or PRAW)\n",
    "\n",
    "Outcome:\n",
    "- CSV files with text, author, date, source, and possibly metadata like upvotes or likes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4c6834-381b-4574-82c6-74233b214d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (you may need to set up Twitter/Reddit credentials separately)\n",
    "!pip install snscrape praw pandas --quiet\n",
    "!pip install -U jupyterlab ipywidgets jupyterlab-widgets\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be62f75f-8927-461c-aa2e-7b85972fd589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will be saved in: ../data/raw\n"
     ]
    }
   ],
   "source": [
    "# Define output path\n",
    "raw_data_path = \"../data/raw\"\n",
    "os.makedirs(raw_data_path, exist_ok=True)\n",
    "print(f\"Data will be saved in: {raw_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587e70ba-0532-4101-a0c0-0ed2df67736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tweepy if not already installed\n",
    "!pip install tweepy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b321ac40-759c-46e5-9f42-09c4b7d4374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 795 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets saved to: ../data/raw/AAPL_twitter_api.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-25 22:54:18+00:00</td>\n",
       "      <td>Best stock traders group out there!\\nupdates +...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-25 22:53:02+00:00</td>\n",
       "      <td>RT @finchat_io: \"Over the long term, it's hard...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-25 22:51:18+00:00</td>\n",
       "      <td>Best stock trade Group out there! \\nFree chatr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-25 22:50:18+00:00</td>\n",
       "      <td>Bast stock group ‚ô•Ô∏è‚ô•Ô∏è\\nDiscordüëåüëå\\n\\nKFC-     h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-25 22:47:40+00:00</td>\n",
       "      <td>Bast stock group ‚ô•Ô∏è‚ô•Ô∏è\\nDiscordüëåüëå\\n\\n MAC-   ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0 2025-03-25 22:54:18+00:00   \n",
       "1 2025-03-25 22:53:02+00:00   \n",
       "2 2025-03-25 22:51:18+00:00   \n",
       "3 2025-03-25 22:50:18+00:00   \n",
       "4 2025-03-25 22:47:40+00:00   \n",
       "\n",
       "                                                text  retweets  likes  \\\n",
       "0  Best stock traders group out there!\\nupdates +...         0      0   \n",
       "1  RT @finchat_io: \"Over the long term, it's hard...        34      0   \n",
       "2  Best stock trade Group out there! \\nFree chatr...         0      0   \n",
       "3  Bast stock group ‚ô•Ô∏è‚ô•Ô∏è\\nDiscordüëåüëå\\n\\nKFC-     h...         0      0   \n",
       "4  Bast stock group ‚ô•Ô∏è‚ô•Ô∏è\\nDiscordüëåüëå\\n\\n MAC-   ht...         0      0   \n",
       "\n",
       "  language   source  \n",
       "0       en  Twitter  \n",
       "1       en  Twitter  \n",
       "2       en  Twitter  \n",
       "3       en  Twitter  \n",
       "4       en  Twitter  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Twitter API Bearer Token (get from https://developer.twitter.com/)\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAJC10AEAAAAAQ8%2FaBip%2BQI2qyhJ76P%2F5I71kvv4%3DoTmwRQrxnzqEQY4nRNsHA5w14cEo65moK9bajb14z9Uz1ljFFv\"\n",
    "\n",
    "# Authenticate\n",
    "client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "# Function to scrape recent tweets for a stock ticker\n",
    "def scrape_twitter_v2(query=\"AAPL stock\", max_results=100):\n",
    "    tweets_data = []\n",
    "    response = client.search_recent_tweets(\n",
    "        query=query,\n",
    "        tweet_fields=[\"created_at\", \"public_metrics\", \"lang\"],\n",
    "        max_results=min(max_results, 100)  # Twitter limits to 100 per request\n",
    "    )\n",
    "\n",
    "    for tweet in response.data:\n",
    "        tweets_data.append({\n",
    "            \"date\": tweet.created_at,\n",
    "            \"text\": tweet.text,\n",
    "            \"retweets\": tweet.public_metrics[\"retweet_count\"],\n",
    "            \"likes\": tweet.public_metrics[\"like_count\"],\n",
    "            \"language\": tweet.lang,\n",
    "            \"source\": \"Twitter\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(tweets_data)\n",
    "\n",
    "# Example: Scrape 100 tweets for AAPL\n",
    "tweet_df = scrape_twitter_v2(\"AAPL stock\", max_results=100)\n",
    "tweet_file_path = os.path.join(\"../data/raw\", \"AAPL_twitter_api.csv\")\n",
    "tweet_df.to_csv(tweet_file_path, index=False)\n",
    "\n",
    "print(f\"Tweets saved to: {tweet_file_path}\")\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c35df87-391b-4985-be9e-06f7c4a37e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit posts saved to: ../data/raw/AAPL_reddit_posts.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-25 18:14:10</td>\n",
       "      <td>Stocks Close Higher for 3rd Straight Day; Tesl...</td>\n",
       "      <td>\\nMajor indexes closed slightly higher Tuesday...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Reddit (stocks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-21 05:30:31</td>\n",
       "      <td>r/Stocks Daily Discussion &amp; Fundamentals Frida...</td>\n",
       "      <td>This is the daily discussion, so anything stoc...</td>\n",
       "      <td>18</td>\n",
       "      <td>330</td>\n",
       "      <td>Reddit (stocks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-20 20:54:15</td>\n",
       "      <td>Short sellers have made $15 billion betting ag...</td>\n",
       "      <td>Short sellers have been cleaning up to start 2...</td>\n",
       "      <td>555</td>\n",
       "      <td>32</td>\n",
       "      <td>Reddit (stocks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-20 02:08:52</td>\n",
       "      <td>A Deeper Dive on Trump's Tariffs and Market Po...</td>\n",
       "      <td>Trump‚Äôs latest trade policies are hammering th...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Reddit (stocks)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-19 15:47:39</td>\n",
       "      <td>The Fate of the S&amp;P's 10 Most Popular Stocks</td>\n",
       "      <td>Almost all of the 10 largest stocks in the Sta...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Reddit (stocks)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                              title  \\\n",
       "0 2025-03-25 18:14:10  Stocks Close Higher for 3rd Straight Day; Tesl...   \n",
       "1 2025-03-21 05:30:31  r/Stocks Daily Discussion & Fundamentals Frida...   \n",
       "2 2025-03-20 20:54:15  Short sellers have made $15 billion betting ag...   \n",
       "3 2025-03-20 02:08:52  A Deeper Dive on Trump's Tariffs and Market Po...   \n",
       "4 2025-03-19 15:47:39       The Fate of the S&P's 10 Most Popular Stocks   \n",
       "\n",
       "                                             content  upvotes  comments  \\\n",
       "0  \\nMajor indexes closed slightly higher Tuesday...        2         4   \n",
       "1  This is the daily discussion, so anything stoc...       18       330   \n",
       "2  Short sellers have been cleaning up to start 2...      555        32   \n",
       "3  Trump‚Äôs latest trade policies are hammering th...        0        10   \n",
       "4  Almost all of the 10 largest stocks in the Sta...        0         5   \n",
       "\n",
       "            source  \n",
       "0  Reddit (stocks)  \n",
       "1  Reddit (stocks)  \n",
       "2  Reddit (stocks)  \n",
       "3  Reddit (stocks)  \n",
       "4  Reddit (stocks)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reddit API credentials (replace with your own)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YYSSSxidNPugOoGJ3eAwwQ\",\n",
    "    client_secret=\"ony_INNIG9cAAkiwZ9Ry-MPzw1d0vQ\",\n",
    "    user_agent=\"vesterai_reddit_scraper\"\n",
    ")\n",
    "\n",
    "# Function to scrape Reddit posts from r/stocks or r/investing\n",
    "def scrape_reddit_posts(subreddit_name=\"stocks\", query=\"AAPL\", limit=50):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "\n",
    "    for submission in subreddit.search(query, limit=limit, sort=\"new\"):\n",
    "        posts.append({\n",
    "            \"date\": datetime.datetime.fromtimestamp(submission.created_utc),\n",
    "            \"title\": submission.title,\n",
    "            \"content\": submission.selftext,\n",
    "            \"upvotes\": submission.score,\n",
    "            \"comments\": submission.num_comments,\n",
    "            \"source\": f\"Reddit ({subreddit_name})\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "# Example: Scrape Reddit posts mentioning AAPL\n",
    "reddit_df = scrape_reddit_posts(\"stocks\", \"AAPL\", limit=50)\n",
    "reddit_file_path = os.path.join(raw_data_path, \"AAPL_reddit_posts.csv\")\n",
    "reddit_df.to_csv(reddit_file_path, index=False)\n",
    "\n",
    "print(f\"Reddit posts saved to: {reddit_file_path}\")\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42986522-82f2-429f-9610-327624ae0e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Media Data Collection Summary:\n",
      "Tweets: 100 ‚Üí saved to ../data/raw/AAPL_twitter_api.csv\n",
      "Reddit posts: 50 ‚Üí saved to ../data/raw/AAPL_reddit_posts.csv\n",
      "\n",
      "Next: Sentiment analysis on these posts in Notebook 03.\n"
     ]
    }
   ],
   "source": [
    "print(\"Social Media Data Collection Summary:\")\n",
    "print(f\"Tweets: {len(tweet_df)} ‚Üí saved to {tweet_file_path}\")\n",
    "print(f\"Reddit posts: {len(reddit_df)} ‚Üí saved to {reddit_file_path}\")\n",
    "print(\"\\nNext: Sentiment analysis on these posts in Notebook 03.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54734ea6-d589-4b57-9f05-f718e4750e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
